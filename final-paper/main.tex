\documentclass[12pt,stu,donotrepeattitle,floatsintext]{apa7}

\usepackage[american]{babel}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{pdfpages}
\usepackage[style=apa,backend=biber]{biblatex}
\usepackage[options]{nohyperref}
\usepackage{url}
\usepackage{appendix}

\DeclareLanguageMapping{american}{american-apa}
\addbibresource{main.bib}
\linespread{2}


\newcommand{\q}[1]{``#1''}
\newcommand{\customsection}[2]{
  \phantomsection
  \section*{#1}\label{#2}
  \addcontentsline{toc}{section}{#1}
}
\newcommand{\customsubsection}[2]{
  \phantomsection
  \subsection*{#1}\label{#2}
  \addcontentsline{toc}{subsection}{#1}
}
\newcommand{\customsubsubsection}[2]{
  \phantomsection
  \subsubsection*{#1}\label{#2}
  \addcontentsline{toc}{subsubsection}{#1}
}

\title{Automated Ice Hockey Player Tracking}

\authorsnames{Douglas Code}
\authorsaffiliations{University of San Diego}
\course{AAI-521: Applied Computer Vision for AI}
\professor{Siamak Aram, Ph.D}
\duedate{December 9, 2024}

\begin{document}
    \maketitle

    \tableofcontents
    \newpage


    \customsection{Introduction}{introduction}

    The ability to track ice hockey players using computer vision presents a number of opportunities.
    Data about player positions and movements can be used for advanced statistical analysis, and team-level positioning and movement can be used to analyze the strategies employed by different teams.
    For live broadcasting, player tracking allows more engaging visualizations by allowing players to be highlighted in real time for commentator discussion or graphical overlays.
    This project aimed to build a model that could take a single game video feed as input and perform four primary tasks:
    \begin{enumerate}
        \item Use object detection to locate the position of all active players in frame.
        \item Track players across frames to maintain consistent identification.
        \item Classify each player as either a skater or goalie.
        \item Classify each player as belonging to either the home team or away team.
    \end{enumerate}
    TODO: Discuss real-time, challenges, technical approach
    

    \customsection{Dataset}{dataset}

    The McGill Hockey Player Tracking Dataset (MHPTD) is a collection of 25 gameplay clips from the National Hockey League (NHL), the highest level North American hockey league.
    These clips were recorded in 30 and 60 frames per second with 1280x720 resolution by a single fixed-position camera for broadcast.
    Each clip captures a continuous sequence of gameplay with no cuts or stoppages in play.
    In all, this represents 82,305 frames with 632,785 instances of a player in the frame~\parencite{mhptd}.

    All clips are annotated using the Computer Vision Annotation Tool (CVAT) in XML format, with tracks for each unique player provided detailing their location in each frame.
    The annotations also include a player ID, whether they are on the home or away team, and whether they are a skater or goalie.
    The position classes (skater, goalie) are significantly imbalanced due to there typically being five skaters and one goalie on the ice at a given time for each team.
    This results in ~8.4\% of the player instances being goalies while the rest are skaters.
    Occlusions are tracked in the annotations as well, with 13.5\% of player instances being occluded in some way.

    \customsection{Preprocessing}{preprocessing}

    To get the annotations into the format expected by YOLO, the coordinates for each player-frame were parsed and normalized to a [0, 1] range relative to the frame's dimensions.
    Each instance was then encoded with one of four classes: home skater, home goalie, away skater, and away goalie.

    Clips were split into train/validation/test datasets using a 60/20/20 split.
    Splitting at the clip level allowed continuity between frames to be preserved for the tracking part of the model, and provided a better test of model efficacy by ensuring that no frames from the validation/test clips had been seen during training.
    To improve the robustness of the model, the training data frames were then augmented using a number of techniques.
    The augmentation techniques that were used are described in Table~\ref{tab:data-aug}~\parencite{yolo_train_docs}.

    \begin{table}[tb]
        \centering
        \renewcommand{\arraystretch}{0.8}
        \begin{tabular}{|l|l|}
            \hline
            \textbf{Method} & \textbf{Description}                                         \\ \hline
            Hue Adjustment        & The hue of the image is modified                       \\ \hline
            Saturation Adjustment & The saturation of the image is modified                \\ \hline
            Value Adjustment      & The brightness of the image is modified                \\ \hline
            Translation           & The image is translated horizontally or vertically     \\ \hline
            Scaling               & The image is scaled to be larger                       \\ \hline
            Flipping              & The image is flipped left-to-right                     \\ \hline
            Mosaic                & Four images are combined into a single composite image \\ \hline
            Erasing               & A portion of the image is erased                       \\ \hline
            Cropping              & Crops a portion of the image                           \\ \hline
        \end{tabular}
        \\[10pt]
        \caption{Data Augmentation Techniques}
        \label{tab:data-aug}
    \end{table}

    All images were resized to a resolution of 640x360, preserving the original aspect ratio.
    This was done to improve training and inference speed while maintaining a high enough resolution for the model to be effective.


    \customsection{Model Architecture}{architecture}

    The model is composed of a two-step pipeline: YOLO for object detection and classification and ByteTrack for object tracking.

    \customsubsection{YOLO}{yolo}

    You Only Look Once (YOLO) is an object-detection approach that performs detection in a single pass rather than using two stages, greatly improving inference speeds.
    This is done by dividing the input image into a grid and predicting bounding boxes and classifications for items centered in each grid cell.
    These bounding boxes are then combined using intersection over union (IOU) thresholds and non-max suppression to get a final predicted bounding for an object ~\parencite{yolo}.
    YOLO was chosen for this application due to its ability to handle real-time inference while maintaining high bounding-box and classification accuracy.
    
    \customsubsection{ByteTrack}{bytetrack}



    \customsection{Training}{training}

    \customsection{Postprocessing}{postprocessing}

    \customsection{Evaluation}{evaluation}

    \customsection{Conclusions}{conclusions}

    \customsubsection{Future Work}{future-work}

    \printbibliography
    \clearpage

    \begin{appendices}
        \section{Appendix A: Project Links}\label{sec:project-links}

        \section{Appendix B: Hyperparameter Configuration}\label{sec:hyperparameters}

        \section{Appendix C: Visualizations}\label{sec:visualizations}

    \end{appendices}

\end{document}